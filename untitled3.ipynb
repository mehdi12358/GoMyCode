{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU1xGbbWiabcdGlaFBY6f/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi12358/GoMyCode/blob/main/untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "1_-c9NYV5brm",
        "outputId": "f72c34b9-1f8f-48ad-f869-3faa678ec73b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    def scrape_wikipedia_page(url):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import beautifulsoup\n",
        "\n",
        "def get_html_content(url):\n",
        "    response =requests.get(url):\n",
        "    return response.content if response.status.code==200:\n",
        "def extract_article_title(html_content)\n",
        "    soup = BeautifulSoup(html_content,'html.parser')\n",
        "    title = soup.find('h1')\n",
        "    return title.text.strip()\n",
        "def extract_article_paragraphs(html_content):\n",
        "    soup = BeautifulSoup(html_content,'html.parser')\n",
        "    paragraphs = soup.find(['p','h1','h2','h3','h4','h5','h6'])\n",
        "    result = {}\n",
        "def collect_internal_links(html_content):\n",
        "    soup = BeautifulSoup(html_content,'html.parser')\n",
        "    links = soup.find_all('a',herf=True)\n",
        "    internal_link = []\n",
        "    for link in links:\n",
        "      href = link['href']\n",
        "      if href.startswith('/wiki/'):\n",
        "          internal_link.append(href)\n",
        "\n",
        "    return internal_link\n",
        "\n",
        "  def scrape_wikipedia_page(url):\n",
        "    html_content = get_html_content(url)\n",
        "\n",
        "    if not html_content:\n",
        "\n",
        "  title = extract_article_title(html_content)\n",
        "  paragraphs = extract_article_paragraphs(html_content)\n",
        "  internal_links = collect_internal_links(html_content)\n",
        "\n",
        "  return{\n",
        "      'title': title\n",
        "      'paragraphs': paragraphs\n",
        "      'internal_links': internal_links\n",
        "if _name_ == \"_main_\":\n",
        "    wikipedia_link = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
        "    scraped_data = scrape_wikipedia_page(wikipedia_link)\n",
        "\n",
        "    if scraped_data:\n",
        "        print(\"Title:\", scraped_data['title'])\n",
        "            for paragraph in paragraph_list:\n",
        "                print(paragraph)\n",
        "        print(\"\\nInternal Links:\")\n",
        "        for link in scraped_data['internal_links']:\n",
        "            print(link)\n",
        "    else:\n",
        "        print(\"Failed to scrape the Wikipedia page.\")\n"
      ]
    }
  ]
}